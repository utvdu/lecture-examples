{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Name Competition with Gradient Boosting\n",
    "\n",
    "This code is provided as supplementary material of the lecture Machine Learning and Optimization in Communications (MLOC).<br>\n",
    "\n",
    "\n",
    "This code illustrates\n",
    "* The use of a gradient boosting classifier from the `scikit-learn` library to perform classification as in the name competition (assign \"+\" if second letter is vowel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load file from data and convert to training set and test set (reading from two distinct files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    y = [line[0] for line in content]    \n",
    "    X = [line[2:].strip() for line in content]\n",
    "    return X,y\n",
    "\n",
    "X_train,y_train = read_file('Names_data_train.txt')\n",
    "X_test,y_test = read_file('Names_data_test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple class that converts the string into numbers and than trains a simple classifier using the gradient boosting technique. The resulting gradient boosting classifier is essentially a rule-based system, where the results are derived from the inputs to the classifier.\n",
    "\n",
    "The main take-away message is that rule-based systems perform extremely well, if the underlying data follows a process that results from simple rules (as are often encountered in practice). For example, medical diagnosis systems follow very clear rules and hence often benefit by training from gradient boosting classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gradient_Boosting_Estimator():\n",
    "    '''\n",
    "    Class for training a gradient boosting, rule-based estimator on the letters\n",
    "    \n",
    "    Parameter is the number of letters of the word to consider\n",
    "    '''    \n",
    "    def __init__( self, letters ):\n",
    "        self.letters = letters\n",
    "        self.gbes = GradientBoostingClassifier()\n",
    "\n",
    "        \n",
    "    # compute the histograms P(Y) (stored in self.Py) and P(x_i|y) (stored in self.Px)\n",
    "    def fit( self, X, y) :       \n",
    "        # convert to numeric entries\n",
    "        ty = np.zeros( len(y) )\n",
    "        for k in range( len(y) ):\n",
    "            if y[k]=='+':\n",
    "                ty[k] = 1\n",
    "        \n",
    "        tX = np.empty( (0, self.letters) )\n",
    "        for mys in X:\n",
    "            if len(mys) < self.letters:             \n",
    "                # add spaces if string is too short\n",
    "                mys += ( ' ' * (self.letters-len(mys) ) )\n",
    "            tX = np.vstack( (tX, [ord(x) for x in mys[0:self.letters] ] ) )\n",
    "    \n",
    "        # fit the classifier (taken from the SciKit-Learn library)\n",
    "        gbes.fit(tX, ty)\n",
    "\n",
    "            \n",
    "    # perform the prediction based on maximizing P(y)\\prod P(x_i|y)\n",
    "    # normally, the prediction would be done in logairthmic domain, but here we just use plain probabilities\n",
    "    def predict(self, X):\n",
    "        rety = ['+' for _ in X]\n",
    "        for idx, elem_X in enumerate(X):\n",
    "            \n",
    "            # add spaces if string is too short\n",
    "            elem_X += ( ' ' * max(0,self.letters-len(elem_X) ) )\n",
    "            elem_numeric = np.array([ord(x) for x in elem_X[0:self.letters]])\n",
    "            \n",
    "            rv = gbes.predict(elem_numeric.reshape(1,-1))\n",
    "            if rv == 0:\n",
    "                rety[idx] = '-'\n",
    "        return rety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Gradient_Boosting_Estimator(10)\n",
    "clf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- predicted as: -      ( Arnold Sommerfeld )\n",
      "- predicted as: -      ( Edward Arnold )\n",
      "+ predicted as: +      ( Gichin Funakoshi )\n",
      "+ predicted as: +      ( John Heartfield )\n",
      "+ predicted as: +      ( Morihei Ueshiba )\n",
      "- predicted as: -      ( Erik Bergman )\n",
      "- predicted as: -      ( Gypsy Rose Lee )\n",
      "- predicted as: -      ( Irene Ryan )\n",
      "+ predicted as: +      ( Sidney Franklin )\n",
      "+ predicted as: +      ( Sid James )\n",
      "- predicted as: -      ( Armstrong Sperry )\n",
      "+ predicted as: +      ( Cicely Courtneidge )\n",
      "+ predicted as: +      ( Jim Davis )\n",
      "+ predicted as: +      ( Count Basie )\n",
      "- predicted as: -      ( Broderick Crawford )\n",
      "+ predicted as: +      ( Bessie Love )\n",
      "+ predicted as: +      ( Dechko Uzunov )\n",
      "+ predicted as: +      ( John Silkin )\n",
      "+ predicted as: +      ( Lucille Ball )\n",
      "+ predicted as: +      ( Leo Arnaud )\n",
      "+ predicted as: +      ( Carmine Coppola )\n",
      "+ predicted as: +      ( Richard Hatfield )\n",
      "+ predicted as: +      ( Mas Oyama )\n",
      "- predicted as: -      ( Stirling Silliphant )\n",
      "- predicted as: -      ( Adrian Borland )\n",
      "+ predicted as: +      ( Jill Dando )\n",
      "+ predicted as: +      ( Rosemary Brown )\n",
      "+ predicted as: +      ( Yun Hyon-seok )\n",
      "- predicted as: -      ( Edward Max Nicholson )\n",
      "+ predicted as: +      ( Hubert Selby )\n",
      "+ predicted as: +      ( Mason Adams )\n",
      "- predicted as: -      ( Elisabeth Domitien )\n",
      "+ predicted as: +      ( Maria Schell )\n",
      "+ predicted as: +      ( Augusto Roa Bastos )\n",
      "+ predicted as: +      ( Jack Valenti )\n",
      "+ predicted as: +      ( Hans Holzer )\n",
      "+ predicted as: +      ( Mariam A. Aleem )\n",
      "- predicted as: -      ( Urs Felber )\n",
      "- predicted as: -      ( Phoebe Snow )\n",
      "+ predicted as: +      ( Terence Spinks )\n",
      "+ predicted as: +      ( Jacqueline Brookes )\n",
      "+ predicted as: +      ( George Jones )\n",
      "+ predicted as: +      ( Gerald Guralnik )\n",
      "+ predicted as: +      ( Paul Robeson )\n",
      "+ predicted as: +      ( Jayne Meadows )\n",
      "+ predicted as: +      ( Marcel Pronovost )\n",
      "+ predicted as: +      ( Harry Wu )\n",
      "+ predicted as: +      ( Jonathan Demme )\n",
      "Prediction errors: 0 (error rate 0.00 %)\n"
     ]
    }
   ],
   "source": [
    "y = clf.predict(X_test)\n",
    "errors = 0\n",
    "for idx,value in enumerate(y_test):\n",
    "    print(value,'predicted as:', y[idx], '     (',X_test[idx],')')\n",
    "    if value != y[idx]:\n",
    "        errors += 1\n",
    "        \n",
    "print('Prediction errors: %d (error rate %1.2f %%)' % (errors, errors/len(y)*100))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 letters: 10 prediction errors (error rate 20.83 %)\n",
      "2 letters: 0 prediction errors (error rate 0.00 %)\n",
      "3 letters: 0 prediction errors (error rate 0.00 %)\n",
      "4 letters: 0 prediction errors (error rate 0.00 %)\n",
      "5 letters: 0 prediction errors (error rate 0.00 %)\n",
      "6 letters: 1 prediction errors (error rate 2.08 %)\n",
      "7 letters: 0 prediction errors (error rate 0.00 %)\n",
      "8 letters: 0 prediction errors (error rate 0.00 %)\n",
      "9 letters: 0 prediction errors (error rate 0.00 %)\n"
     ]
    }
   ],
   "source": [
    "# find optimal number of errors\n",
    "for letter in range(1,10):    \n",
    "    clf = Gradient_Boosting_Estimator(letter)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y = clf.predict(X_test)\n",
    "    \n",
    "    errors = 0\n",
    "    for idx,k in enumerate(y_test):    \n",
    "        if k != y[idx]:\n",
    "            errors += 1\n",
    "        \n",
    "    print('%d letters: %d prediction errors (error rate %1.2f %%)' % (letter, errors,errors*100/len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with 5 letters\n",
    "clf = Gradient_Boosting_Estimator(5)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+']\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict(['Xavier Jones']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
